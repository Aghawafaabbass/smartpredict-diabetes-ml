 ğŸ”¬ SmartPredict: Machine Learning for Early Diabetes Prediction  

## ğŸ“Œ Overview  
Early detection of **diabetes** can save lives by preventing severe complications like heart disease, kidney failure, and nerve damage.  
This project compares **five machine learning algorithms** on the **Pima Indians Diabetes Dataset** to forecast diabetes:  

- ğŸ“ˆ Logistic Regression (LR)  
- ğŸŒ³ Decision Tree (DT)  
- ğŸŒ² Random Forest (RF)  
- ğŸ‘¥ K-Nearest Neighbors (KNN)  
- ğŸ“Š Support Vector Machine (SVM)  

ğŸ‘‰ **Key finding**: Random Forest achieved the highest performance, showing the strength of ensemble methods in healthcare prediction.  

---

## ğŸ“‚ Dataset  
- **Source**: [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)  
- **Records**: 768 Pima Indian women  
- **Features**:  
  - Pregnancies  
  - Glucose  
  - Blood Pressure  
  - Skin Thickness  
  - Insulin  
  - BMI  
  - Diabetes Pedigree Function  
  - Age  
- **Target**: Diabetic (1) / Non-Diabetic (0)  

---

## âš™ï¸ Methodology  
1. **Data Preprocessing**  
   - Replaced invalid `0` values with mean values  
   - Standardized features using `StandardScaler`  
   - Train-test split (80/20)  

2. **Model Training**  
   - Implemented with `scikit-learn`  
   - 5-fold cross-validation for reliability  

3. **Evaluation Metrics**  
   - Accuracy (%)  
   - Confusion Matrix  
   - ROC Curve  
   - Feature Importance (Random Forest)  

---

## ğŸ“Š Results  

### ğŸ”¹ Cross-Validation Accuracy
| Algorithm            | Accuracy (%) |
|----------------------|--------------|
| ğŸŒ² Random Forest     | **77.37**    |
| ğŸ“Š Support Vector Machine | 76.50    |
| ğŸ“ˆ Logistic Regression   | 76.06    |
| ğŸ‘¥ K-Nearest Neighbors   | 74.44    |
| ğŸŒ³ Decision Tree         | 72.80    |

---

### ğŸ”¹ Test Set Accuracy
| Algorithm            | Accuracy (%) |
|----------------------|--------------|
| ğŸŒ² Random Forest     | **75.97**    |
| ğŸ“Š Support Vector Machine | 75.50    |
| ğŸ“ˆ Logistic Regression   | 75.32    |
| ğŸŒ³ Decision Tree         | 74.68    |
| ğŸ‘¥ K-Nearest Neighbors   | 69.48    |

âœ… **Best Model**: Random Forest  

---

## ğŸ“ˆ Visualizations  
- ğŸ”¥ Correlation Heatmap of Features  
- ğŸ“Š Bar Plots of Accuracy  
- ğŸŒ€ Confusion Matrices for each model  
- ğŸ“‰ ROC Curves (AUC Comparison)  
- ğŸŒ² Feature Importance (Random Forest)  
- ğŸ“š Learning & Calibration Curves  

---

## â–¶ï¸ How to Run the Notebook  

You can run the project in **Google Colab** (recommended) or locally on your system.  

### ğŸ”¹ Option 1: Run in Google Colab  
1. Open [Google Colab](https://colab.research.google.com/).  
2. Click on **File > Upload Notebook**.  
3. Upload the `.ipynb` file from this repo.  
4. Run the cells one by one (press `Shift + Enter`).  

### ğŸ”¹ Option 2: Run Locally  
1. Clone this repository:  
   bash
   git clone https://github.com/YourUsername/your-repo-name.git
   cd your-repo-name
Install dependencies:

bash

pip install -r requirements.txt
Launch Jupyter Notebook:

bash

jupyter notebook
Open diabetes_analysis.ipynb and run the cells.

## ğŸ› ï¸ Tech Stack  
- **Language**: Python 3.9+  
- **Environment**: Jupyter Notebook / Google Colab  
- **Libraries**:  
  - `pandas`  
  - `numpy`  
  - `scikit-learn`  
  - `matplotlib`  
  - `seaborn`  

---

## ğŸ“‚ Repository Structure  
â”œâ”€â”€ data/
â”‚ â””â”€â”€ diabetes.csv # Dataset
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ diabetes_analysis.ipynb # Jupyter Notebook
â”œâ”€â”€ results/
â”‚ â””â”€â”€ visualizations/ # Graphs & plots
â”œâ”€â”€ README.md # Documentation
â””â”€â”€ LICENSE # MIT License


---

## âœ¨ Key Takeaways  
- âœ… **Random Forest** performed best (highest accuracy & stability)  
- ğŸ“ˆ Logistic Regression = great **interpretability** for healthcare  
- âŒ KNN = weakest model, sensitive to scaling & noise  
- â­ Important predictors: **Glucose, BMI, Age**  

---

## ğŸ”® Future Work  
- Implement **Deep Learning models** (e.g., Neural Networks)  
- Handle **class imbalance** using SMOTE or weighted loss  
- Improve **explainability** of ensemble methods for clinicians  

---

## ğŸ‘¨â€ğŸ’» Author  
**Agha Wafa Abbas**  
- Lecturer, School of Computing, Arden University, UK  
- Lecturer, Pearson, London, UK  
- Lecturer, IVY College of Management Sciences, Pakistan  
- ğŸ“§ Email: awabbas@arden.ac.uk  

---


---

## âš–ï¸ License  
This project is licensed under the **MIT License** â€“ see the [LICENSE](./LICENSE) file for details. 

 üî¨ SmartPredict: Machine Learning for Early Diabetes Prediction  

## üìå Overview  
Early detection of **diabetes** can save lives by preventing severe complications like heart disease, kidney failure, and nerve damage.  
This project compares **five machine learning algorithms** on the **Pima Indians Diabetes Dataset** to forecast diabetes:  

- üìà Logistic Regression (LR)  
- üå≥ Decision Tree (DT)  
- üå≤ Random Forest (RF)  
- üë• K-Nearest Neighbors (KNN)  
- üìä Support Vector Machine (SVM)  

üëâ **Key finding**: Random Forest achieved the highest performance, showing the strength of ensemble methods in healthcare prediction.  

---

## üìÇ Dataset  
- **Source**: [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)  
- **Records**: 768 Pima Indian women  
- **Features**:  
  - Pregnancies  
  - Glucose  
  - Blood Pressure  
  - Skin Thickness  
  - Insulin  
  - BMI  
  - Diabetes Pedigree Function  
  - Age  
- **Target**: Diabetic (1) / Non-Diabetic (0)  

---

## ‚öôÔ∏è Methodology  
1. **Data Preprocessing**  
   - Replaced invalid `0` values with mean values  
   - Standardized features using `StandardScaler`  
   - Train-test split (80/20)  

2. **Model Training**  
   - Implemented with `scikit-learn`  
   - 5-fold cross-validation for reliability  

3. **Evaluation Metrics**  
   - Accuracy (%)  
   - Confusion Matrix  
   - ROC Curve  
   - Feature Importance (Random Forest)  

---

## üìä Results  

### üîπ Cross-Validation Accuracy
| Algorithm            | Accuracy (%) |
|----------------------|--------------|
| üå≤ Random Forest     | **77.37**    |
| üìä Support Vector Machine | 76.50    |
| üìà Logistic Regression   | 76.06    |
| üë• K-Nearest Neighbors   | 74.44    |
| üå≥ Decision Tree         | 72.80    |

---

### üîπ Test Set Accuracy
| Algorithm            | Accuracy (%) |
|----------------------|--------------|
| üå≤ Random Forest     | **75.97**    |
| üìä Support Vector Machine | 75.50    |
| üìà Logistic Regression   | 75.32    |
| üå≥ Decision Tree         | 74.68    |
| üë• K-Nearest Neighbors   | 69.48    |

‚úÖ **Best Model**: Random Forest  

---

## üìà Visualizations  
- üî• Correlation Heatmap of Features  
- üìä Bar Plots of Accuracy  
- üåÄ Confusion Matrices for each model  
- üìâ ROC Curves (AUC Comparison)  
- üå≤ Feature Importance (Random Forest)  
- üìö Learning & Calibration Curves  

---

## üõ†Ô∏è Tech Stack  
- **Language**: Python 3.9+  
- **Environment**: Jupyter Notebook / Google Colab  
- **Libraries**:  
  - `pandas`  
  - `numpy`  
  - `scikit-learn`  
  - `matplotlib`  
  - `seaborn`  

---

## üìÇ Repository Structure  
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îî‚îÄ‚îÄ diabetes.csv # Dataset
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îî‚îÄ‚îÄ diabetes_analysis.ipynb # Jupyter Notebook
‚îú‚îÄ‚îÄ results/
‚îÇ ‚îî‚îÄ‚îÄ visualizations/ # Graphs & plots
‚îú‚îÄ‚îÄ README.md # Documentation
‚îî‚îÄ‚îÄ LICENSE # MIT License

yaml
Copy
Edit

---

## ‚ú® Key Takeaways  
- ‚úÖ **Random Forest** performed best (highest accuracy & stability)  
- üìà Logistic Regression = great **interpretability** for healthcare  
- ‚ùå KNN = weakest model, sensitive to scaling & noise  
- ‚≠ê Important predictors: **Glucose, BMI, Age**  

---

## üîÆ Future Work  
- Implement **Deep Learning models** (e.g., Neural Networks)  
- Handle **class imbalance** using SMOTE or weighted loss  
- Improve **explainability** of ensemble methods for clinicians  

---

## üë®‚Äçüíª Author  
**Agha Wafa Abbas**  
- Lecturer, School of Computing, Arden University, UK  
- Lecturer, Pearson, London, UK  
- Lecturer, IVY College of Management Sciences, Pakistan  
- üìß Email: awabbas@arden.ac.uk  

---

## üìú Citation  
If you use this work, please cite:  

> Agha Wafa Abbas, *SmartPredict: A Comparative Exploration of Machine Learning for Early Diabetes Forecasting*  

---

## ‚öñÔ∏è License  
This project is licensed under the **MIT License** ‚Äì see the [LICENSE](./LICENSE) file for details. 
